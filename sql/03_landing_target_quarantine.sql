use role accountadmin;
use database sales_db;
use warehouse ingest_wh;
use schema raw;

// create pipe for auto ingestion in landing table

CREATE OR REPLACE PIPE orders_landing_pipe
AUTO_INGEST = TRUE
AS
COPY INTO STG_ORDERS_LANDING (
    ORDER_ID,
    CUSTOMER_ID,
    PRODUCT_ID,
    QUANTITY,
    PAYMENT_METHOD,
    CREATED_AT
)
FROM @s3_sales_stage
FILE_FORMAT = (FORMAT_NAME = sales_csv_format)
ON_ERROR = 'ABORT_STATEMENT';

-- Notification setup --
DESC PIPE orders_landing_pipe; // copy notification_channel from description of pipe.This is an SQS ARN generated by Snowflake.

-- Configure S3 Event Notification (AWS side) In AWS S3 bucket → Properties → Event Notifications
--SQS ARN: use Snowpipe notification_channel // This connects S3 → Snowpipe.

// Automatically happens = S3 → Snowpipe → STG_ORDERS_LANDING



-- Automate the logic of loading into target_table and rejected_table using (Snowpipe + Stream + Task)
    
// 1.Snowpipe loads into STG_ORDERS_LANDING
// 2.Stream captures new rows (CDC = only newly arrives rows)
// 3.Task runs only when stream has data
// 4.Inserts into target + quarantine

// Note : Task wakes up every 5 minutes, But executes only if stream has data, Otherwise it skips execution (no warehouse usage).

-- In order to do that create stream first of landing table --
CREATE OR REPLACE STREAM orders_stream
ON TABLE STG_ORDERS_LANDING ;

CREATE OR REPLACE TASK orders_transform_task
WAREHOUSE = TRANSFORM_WH
SCHEDULE = '10 MINUTE'
WHEN
  SYSTEM$STREAM_HAS_DATA('STG_ORDERS_STREAM') // If table has new data then only data is loaded.
AS
BEGIN

    INSERT INTO target_table_orders ( // valid rows -> target
        ORDER_ID,
        CUSTOMER_ID,
        PRODUCT_ID,
        QUANTITY,
        PAYMENT_METHOD,
        CREATED_AT
    )
    SELECT
        ORDER_ID,
        CUSTOMER_ID,
        PRODUCT_ID,
        TRY_TO_NUMBER(QUANTITY) AS QUANTITY,
        PAYMENT_METHOD,
        TRY_TO_TIMESTAMP_NTZ(CREATED_AT) AS CREATED_AT
    FROM STG_ORDERS_LANDING
    WHERE
        TRY_TO_NUMBER(QUANTITY) IS NOT NULL
        AND TRY_TO_TIMESTAMP_NTZ(CREATED_AT) IS NOT NULL;
     
    INSERT INTO raw_sales_orders_rejected // invalid rows -> reject
        (
          ORDER_ID, CUSTOMER_ID, PRODUCT_ID, QUANTITY, PAYMENT_METHOD,                 CREATED_AT, LOAD_TIME, ERROR_REASON
        )
    SELECT
          ORDER_ID,
          CUSTOMER_ID,
          PRODUCT_ID,
          QUANTITY,
          PAYMENT_METHOD,
          CREATED_AT,
          LOAD_TIME,
          CONCAT(
            IFF(TRY_TO_NUMBER(QUANTITY) IS NULL, 'Invalid QUANTITY; ', ''),
            IFF(TRY_TO_TIMESTAMP_NTZ(CREATED_AT) IS NULL, 'Invalid                       CREATED_AT; ', '')
                ) AS ERROR_REASON // iff is like if else and case when.
    FROM STG_ORDERS_LANDING
    WHERE
          TRY_TO_NUMBER(QUANTITY) IS NULL
          OR TRY_TO_TIMESTAMP_NTZ(CREATED_AT) IS NULL;


ALTER TASK orders_transform_task RESUME; // Resume task so it will be ready to run 


-- Final step is to monitor SNOWPIPE & File Loading in tables 

SELECT *
FROM TABLE(
  INFORMATION_SCHEMA.PIPE_USAGE_HISTORY(      //Snowpipe monitoring
    DATEADD('HOUR', -24, CURRENT_TIMESTAMP()),
    CURRENT_TIMESTAMP()
  )
);

SELECT *
FROM TABLE(
  INFORMATION_SCHEMA.COPY_HISTORY(        // File load tracking
    TABLE_NAME => 'STG_ORDERS_LANDING',
    START_TIME => DATEADD('HOUR', -24, CURRENT_TIMESTAMP())
  )
);








